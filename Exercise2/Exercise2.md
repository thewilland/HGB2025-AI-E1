# Activity 1 
#### Considering the above part Debezium CDC with PostgreSQL and Kafka, explain with your own words what it does and why it is a relevant software architecture for Big Data in the AI era and for which use cases.

What the combination of PostgreSQL, Debezium and Kafka do is solve the issues that arise from using PostgreSQL directly as it is not good for multiple concurrent consumers, complex analytical queries and low processing time. So for cases such as fraud detection (as will be discussed later in Activity 3), anomaly detection, real-time recommender systems, real-time health monitoring systems where such things are very important a direct PostgreSQL usage does not work. <br>

To explain how exactly such an architecture works I will take an example where we have a factory with many machines operating concurrently and sensors on each machine tracking their state in real time (a lot of data). So what is being written to the PostgreSQL database are the sensor readings (for example temperature and sound frequency). Then what Debezium does is read the logs from PostgreSQL (so it does not query the database and only reads the changes in the logs) and converts them to events which are then publishes them to Kafka topics. Now several consumers like a dashboard, a fraud detection model and a model training pipeline can recieve the data quickly and reliably and anomalies can be detected very soon and dangerous malfunctions can be prevented. <br>

This is exactly why it is relevant when dealing with Big Data and AI models as we want appropriate actions to be taken quickly and reliably and not find out half an hour later that a machine has exploded and somebody stole all money from your bank account.


# Activity 2

## Part 1
#### In a simple use case where sensor readings need to be processed every 10 minutes to calculate the average temperature over that time window, describe which software architecture would be most appropriate for fetching the data from PostgreSQL, and explain the rationale behind your choice.

In this case a direct PostgreSQL usage would be enough. Since there is no time sensitivity (if we find out 10 minutes later that temperature increased by 2 degrees nothing horrible would happen as humans can very much withstand slight temperature changes :) ), no huge data load, a very simple query (average) and only a single consumer - Debezium and Kafka are completely unnecessary. 

## Part 2
#### From the architectural choice made in ```Part 1```, implement the solution to consume and processing the data generated by the ```temperature_data_producer.py``` file (revise its features!). The basic logic from the file ```temperature_data_consumer.py``` should be extended with the conection to data source defined in ```Part 1```'s architecture..

``` shell
docker compose up -d

source venv/bin/activate
python3 temperature_data_producer.py

# in a different terminal

source venv/bin/activate
python3 temperature_data_consumer.py
```

## Part 3
#### Discuss the proposed architecture in terms of resource efficiency, operability, and deployment complexity. This includes analyzing how well the system utilizes compute, memory, and storage resources; how easily it can be operated, monitored, and debugged in production.

This architecture uses the recources it has efficiently as simple infrequent queries and low data amount are what PostgreSQL handles easily. And since I am not adding anything else I do not get any additional complexity and overhead from things that are not even being utilised to their full capacity. Due to this low complexity it is very easy to operate and monitor. If something goes wrong it can be immediately visible because in that case the average temperature simply does not get printed. And in case of something going wrong it is easy to just stop the whole process and inspect what went wrong (there would be absolutely no real damage from stopping all the processes and resuming after a while). There are also very few places where anything could go wrong so pinpointing the exact one and fixing it would not take a long time. Due to the same reason (few components) deploying the whole thing is also not that diffucult.


# Activity 3
## Part 1
#### Describe which software architecture would be most appropriate for fetching the data from PostgreSQL and generate alerts in real-time. Explain the rationale behind your choice.

This is the case where PostgreSQL + Debezium + Kafka is very appropriate and necessary. We have the specific reasons to use it: large data volume, multiple consumers, a necessity for real-time detection (same as with the machine in a factory example it is incredibly important to detect a suspicious pattern and act on that before it is too late). This can simply not be achieved with just PostgreSQL

## Part 2
#### From the architectural choice made in ```Part 1```, implement the 'consumer' to fetch and process the records generated by the ```fraud_data_producer.py``` file (revise its features!). The basic logic from the files ```fraud_consumer_agent1.py.py``` and ```fraud_consumer_agent2.py.py``` should be extended with the conection to data source defined in ```Part 1```'s architecture.

``` shell
pip install kafka-python-ng

curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "fraud-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "database.hostname": "postgres",
      "database.port": "5432",
      "database.user": "postgres",
      "database.password": "postgrespw",
      "database.dbname": "mydb",
      "topic.prefix": "frauddb",
      "plugin.name": "pgoutput",
      "slot.name": "fraudslot",
      "tasks.max": "1"
    }
  }'

  docker exec -it kafka kafka-topics.sh \
  --bootstrap-server kafka:9092 \
  --list

source venv/bin/activate
python3 fraud_data_producer.py

# new terminal
source venv/bin/activate
python3 fraud_consumer_agent1.py

# new terminal
source venv/bin/activate
python3 fraud_consumer_agent2.py

```

## Part 3
#### Discuss the proposed architecture in terms of resource efficiency, operability, maintainability, deployment complexity, and overall performance and scalability. This includes discussing how well the system utilizes compute, memory, and storage resources; how easily it can be operated, monitored, and debugged in production; how maintainable and evolvable the individual components are over time; the effort required to deploy and manage the infrastructure; and the systemâ€™s ability to sustain increasing data volumes, higher ingestion rates, and a growing number of fraud detection agents without degradation of latency or reliability.

In this architecture there are no longer any queries being executed through PostgreSQL, all the fraud detection is being done in python scripts which are more effecient for complex analytics like he ones being done here. Multiple consumers can now also be easily added without restarting the whole process or other consumers. So if I am somehow unsatisfied with the performance of my current models I can develop a new one and make it listen to my existing topic without stopping my other models and risking fraud occuring without my knowledge. Additionally if my transaction system expands and I get way more data flow - I just add more partitions and consumers. <br>

The deployment is rather complicated as I myself have ran into multiple problems (like using the correct version of Kafka that works for Python 13.3, making sure the consumers are listening to the correct topic, making sure that the Debezium connector works). This architecture is conceptually sophisticated and it is more difficult to grasp what is truly going on and at which point something is failing. So while for Activity 2 if the average is not printing and no ther error - the part that prints is wrong, but in Acitivty 3 I had the consumer run, print that it has started but no futher messages - Is my connection wrong? Is the algorithm wrong? Is the topic name wrong? It is more difficult to troubleshoot what went wrong.

## Part 4
#### Compare the proposed architecture to Exercise 3 from previous lecture where the data from PostgreSQL was loaded to Spark (as a consumer) using the JDBC connector. Discuss both approaches at least in terms of performance, resource efficiency, and deployment complexity. 

In the case of Spark and JDBC while all the complex analysis is being done by Spark, I am still loading data directly from PostgreSQL, not it's logs, so if my database does not change rapidly - that is fine. But for a case like this with 1000 transactions per second I would have to load the data from PostgreSQL each time and that would quickly get out of hand especially since there is additional overhead from JDBC and Spark. There are however fewer parts in that architecture - so it is simplier to deploy.