{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30fd8196",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "1. cd ecommerce\n",
    "2. python3 dataset_generator.py\n",
    "3. wc -l orders_1M.csv\n",
    "4. Here I moved the csv file to data because for some reason it could not find it when it was in ecommerce\n",
    "``` sql\n",
    "DROP TABLE IF EXISTS orders;\n",
    "\n",
    "CREATE TABLE orders (\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  customer_name TEXT,\n",
    "  product_category TEXT,\n",
    "  quantity INTEGER,\n",
    "  price_per_unit FLOAT,\n",
    "  order_date DATE,\n",
    "  country TEXT\n",
    ");\n",
    "\n",
    "\\COPY orders(customer_name, product_category, quantity, price_per_unit, order_date, country)\n",
    "FROM '/data/orders_1M.csv' DELIMITER ',' CSV HEADER;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83120a81",
   "metadata": {},
   "source": [
    "### A. What is the single item with the highest price_per_unit?\n",
    "``` sql \n",
    "SELECT product_category, price_per_unit\n",
    "FROM orders\n",
    "ORDER BY price_per_unit DESC\n",
    "LIMIT 1;\n",
    "```\n",
    "\n",
    "| product_category | price_per_unit |\n",
    "|----|----|\n",
    "|Automotive|2000|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d587c",
   "metadata": {},
   "source": [
    "### B. What are the top 3 products with the highest total quantity sold across all orders?\n",
    "\n",
    "``` sql\n",
    "SELECT product_category, SUM(quantity) AS total_quantity_sold\n",
    "FROM orders\n",
    "GROUP BY product_category\n",
    "ORDER BY total_quantity_sold DESC\n",
    "LIMIT 3;\n",
    "```\n",
    "| product_category | total_quantity_sold |\n",
    "|------------------|---------------------|\n",
    "| Health & Beauty  |              300842|\n",
    "| Electronics      |              300804|\n",
    "| Toys             |              300598|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c292ac",
   "metadata": {},
   "source": [
    "### C. What is the total revenue per product category? (Revenue = price_per_unit Ã— quantity)\n",
    "\n",
    "``` sql\n",
    "SELECT product_category, SUM(price_per_unit * quantity) AS total_revenue\n",
    "FROM orders\n",
    "GROUP BY product_category\n",
    "ORDER BY total_revenue DESC;\n",
    "```\n",
    "\n",
    "| product_category |   total_revenue |   \n",
    "|------------------|--------------------|\n",
    "| Automotive       | 306589798.86000013|\n",
    "| Electronics      | 241525009.44999987|\n",
    "| Home & Garden    |  78023780.09000014|\n",
    "| Sports           |  61848990.83000007|\n",
    "| Health & Beauty  | 46599817.890000135|\n",
    "| Office Supplies  |  38276061.63999981|\n",
    "| Fashion          |  31566368.22000009|\n",
    "| Toys             | 23271039.020000048|\n",
    "| Grocery          |  15268355.66000005|\n",
    "| Books            | 12731976.040000036|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242a60e",
   "metadata": {},
   "source": [
    "### D. Which customers have the highest total spending?\n",
    "\n",
    "``` sql\n",
    "SELECT customer_name, SUM(price_per_unit * quantity) AS total_spent\n",
    "FROM orders\n",
    "GROUP BY customer_name\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "| customer_name  |    total_spent  |  \n",
    "|----------------|-------------------|\n",
    "| Carol Taylor   | 991179.1800000002|\n",
    "| Nina Lopez     | 975444.9500000002|\n",
    "| Daniel Jackson |         959344.48|\n",
    "| Carol Lewis    | 947708.5699999998|\n",
    "| Daniel Young   | 946030.1400000001|\n",
    "| Alice Martinez | 935100.0200000006|\n",
    "| Ethan Perez    | 934841.2400000001|\n",
    "| Leo Lee        | 934796.4799999995|\n",
    "| Eve Young      | 933176.8600000003|\n",
    "| Ivy Rodriguez  | 925742.6400000006|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18ae8b",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "The first big issue is with the query itself -\n",
    "\n",
    "``` sql\n",
    "SELECT COUNT(*)\n",
    "FROM people_big p1\n",
    "JOIN people_big p2\n",
    "  ON p1.country = p2.country;\n",
    "```\n",
    "\n",
    "What this query does is self join the table which explodes its size and then counts the amount of rows. Example: \n",
    "\n",
    "|name|country|\n",
    "|--|--|\n",
    "|Jonathan S.| UK|\n",
    "|Martin B.| UK|\n",
    "|Pat G.| Latvia|\n",
    "|Andrej M.| Latvia|\n",
    "\n",
    "After join -\n",
    "\n",
    "|name 1|name 2| country |\n",
    "|--|--|--|\n",
    "|Jonathan S.| Martin B.|UK|\n",
    "|Jonathan S.| Jonathan S.|UK|\n",
    "|Martin B.|Jonathan S.| UK|\n",
    "|Martin B.| Martin B.|UK|\n",
    "|Pat G.| Andrej M.|Latvia|\n",
    "|Pat G.| Pat G.|Latvia|\n",
    "|Andrej M.| Pat G.|Latvia|\n",
    "|Andrej M.| Andrej M.|Latvia|\n",
    "\n",
    "This is completely unnecessary and can be done more nicely and cleverly -\n",
    "\n",
    "``` sql\n",
    "SELECT SUM(count * count)\n",
    "FROM (\n",
    "  SELECT country, COUNT(*) AS count\n",
    "  FROM people_big\n",
    "  GROUP BY country\n",
    ") temp;\n",
    "```\n",
    "\n",
    "This has the same output except it avoids the costly self join and simply counts the amount of people from each country, squares them (same as the join creating pairs) abd then sums those up. <br>\n",
    "\n",
    "Of course it is not really possibe to enforce smart query making rules on users. But what is possible is limiting the amount of time a query can execute for to avoid some badly written queries. <br>\n",
    "\n",
    "Apart from that usual solutions like indexing or views would not help too much because the main problem is that this query is an analytical one - not what OLTP is optimised for. OLTP is optimised for many small simple concurrent queries. OLAP on the other hand is optimised for infrequent but complex queries. So a good idea in this case would be to switch to an OLAP database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221114e",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a753a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.makedirs(\"/tmp/spark-events\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f610e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading people_big from PostgreSQL ===\n",
      "Rows loaded: 1000000\n",
      "Load time: 25.77 seconds\n",
      "\n",
      "=== Query (a): AVG salary per department ===\n",
      "+------------------+----------+\n",
      "|department        |avg_salary|\n",
      "+------------------+----------+\n",
      "|Workforce Planning|85090.82  |\n",
      "|Web Development   |84814.36  |\n",
      "|UX Design         |84821.2   |\n",
      "|UI Design         |85164.64  |\n",
      "|Treasury          |84783.27  |\n",
      "|Training          |85148.1   |\n",
      "|Tax               |85018.57  |\n",
      "|Sustainability    |85178.99  |\n",
      "|Supply Chain      |84952.89  |\n",
      "|Subscriptions     |84899.19  |\n",
      "+------------------+----------+\n",
      "\n",
      "Query (a) time: 18.29 seconds\n",
      "\n",
      "=== Query (b): Nested aggregation ===\n",
      "+------------+-----------------+\n",
      "|country     |avg_salary       |\n",
      "+------------+-----------------+\n",
      "|Egypt       |87382.229633112  |\n",
      "|Kuwait      |87349.3517377211 |\n",
      "|Saudi Arabia|87348.80512175433|\n",
      "|Panama      |87345.00623707911|\n",
      "|Denmark     |87328.03514120901|\n",
      "|Jamaica     |87305.437352083  |\n",
      "|Lebanon     |87292.76891750695|\n",
      "|Turkey      |87290.69043798617|\n",
      "|Malaysia    |87253.78746341489|\n",
      "|Kazakhstan  |87251.74274968785|\n",
      "+------------+-----------------+\n",
      "\n",
      "Query (b) time: 14.27 seconds\n",
      "\n",
      "=== Query (c): Top 10 salaries ===\n",
      "+------+----------+---------+------+----------------+------+------------+\n",
      "|id    |first_name|last_name|gender|department      |salary|country     |\n",
      "+------+----------+---------+------+----------------+------+------------+\n",
      "|764650|Tim       |Jensen   |Male  |Analytics       |160000|Bulgaria    |\n",
      "|10016 |Anastasia |Edwards  |Female|Analytics       |159998|Kuwait      |\n",
      "|754528|Adrian    |Young    |Male  |Game Analytics  |159997|UK          |\n",
      "|240511|Diego     |Lopez    |Male  |Game Analytics  |159995|Malaysia    |\n",
      "|893472|Mariana   |Cook     |Female|People Analytics|159995|South Africa|\n",
      "|359891|Mariana   |Novak    |Female|Game Analytics  |159992|Mexico      |\n",
      "|53102 |Felix     |Taylor   |Male  |Data Science    |159989|Bosnia      |\n",
      "|768143|Teresa    |Campbell |Female|Game Analytics  |159988|Spain       |\n",
      "|729165|Antonio   |Weber    |Male  |Analytics       |159987|Moldova     |\n",
      "|952549|Adrian    |Harris   |Male  |Analytics       |159986|Georgia     |\n",
      "+------+----------+---------+------+----------------+------+------------+\n",
      "\n",
      "Query (c) time: 17.69 seconds\n",
      "\n",
      "=== Query (d): Heavy self-join COUNT (DANGEROUS) ===\n",
      "Join count: 10983941260\n",
      "Query (d) time: 138.96 seconds\n",
      "\n",
      "=== Query (d-safe): Join-equivalent rewrite ===\n",
      "+-----------+\n",
      "|total_pairs|\n",
      "+-----------+\n",
      "|10983941260|\n",
      "+-----------+\n",
      "\n",
      "Query (d-safe) time: 5.49 seconds\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0. Imports & Spark session\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "import builtins  # <-- IMPORTANT\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    avg,\n",
    "    round as spark_round,   # Spark round ONLY for Columns\n",
    "    count,\n",
    "    col,\n",
    "    sum as _sum\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PostgresVsSparkBenchmark\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.2\")\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\n",
    "    .config(\"spark.history.fs.logDirectory\", \"/tmp/spark-events\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .config(\"spark.default.parallelism\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# ============================================\n",
    "# 1. JDBC connection config\n",
    "# ============================================\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://pg-bigdata:5432/postgres\"\n",
    "jdbc_props = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# 2. Load data from PostgreSQL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Loading people_big from PostgreSQL ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_big = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"people_big\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "\n",
    "# Force materialization\n",
    "row_count = df_big.count()\n",
    "\n",
    "print(f\"Rows loaded: {row_count}\")\n",
    "print(\"Load time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# Register temp view\n",
    "df_big.createOrReplaceTempView(\"people_big\")\n",
    "\n",
    "# ============================================\n",
    "# 3. Query (a): Simple aggregation\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (a): AVG salary per department ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_a = (\n",
    "    df_big\n",
    "    .groupBy(\"department\")\n",
    "    .agg(spark_round(avg(\"salary\"), 2).alias(\"avg_salary\"))\n",
    "    .orderBy(\"department\", ascending=False)\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "q_a.collect()\n",
    "q_a.show(truncate=False)\n",
    "print(\"Query (a) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 4. Query (b): Nested aggregation\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (b): Nested aggregation ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_b = spark.sql(\"\"\"\n",
    "SELECT country, AVG(avg_salary) AS avg_salary\n",
    "FROM (\n",
    "    SELECT country, department, AVG(salary) AS avg_salary\n",
    "    FROM people_big\n",
    "    GROUP BY country, department\n",
    ") sub\n",
    "GROUP BY country\n",
    "ORDER BY avg_salary DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "q_b.collect()\n",
    "q_b.show(truncate=False)\n",
    "print(\"Query (b) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 5. Query (c): Sorting + Top-N\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (c): Top 10 salaries ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_c = (\n",
    "    df_big\n",
    "    .orderBy(col(\"salary\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "q_c.collect()\n",
    "q_c.show(truncate=False)\n",
    "print(\"Query (c) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 6. Query (d): Heavy self-join (COUNT only)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (d): Heavy self-join COUNT (DANGEROUS) ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_d = (\n",
    "    df_big.alias(\"p1\")\n",
    "    .join(df_big.alias(\"p2\"), on=\"country\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "print(\"Join count:\", q_d)\n",
    "print(\"Query (d) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 7. Query (d-safe): Join-equivalent rewrite\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (d-safe): Join-equivalent rewrite ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "grouped = df_big.groupBy(\"country\").agg(count(\"*\").alias(\"cnt\"))\n",
    "\n",
    "q_d_safe = grouped.select(\n",
    "    _sum(col(\"cnt\") * col(\"cnt\")).alias(\"total_pairs\")\n",
    ")\n",
    "\n",
    "q_d_safe.collect()\n",
    "q_d_safe.show()\n",
    "print(\"Query (d-safe) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 8. Cleanup\n",
    "# ============================================\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088f58e",
   "metadata": {},
   "source": [
    "## 1. What the Spark code does\n",
    "\n",
    "### 1. Spark session initialisation\n",
    "- Starting a Spark driver\n",
    "- Creating a local environment\n",
    "- Configuring JDBC connection\n",
    "\n",
    "### 2. Loading data from Postgres\n",
    "- Connecting to PostgreSQL\n",
    "- Reading the table\n",
    "- Converting to Spark dataframe\n",
    "- Stores it in memory\n",
    "- Creates a temporary view which allows to use SQL-style queries\n",
    "\n",
    "After this step PostgreSQL is no longer involved in query execution. All queries run inside Spark\n",
    "\n",
    "### 3. Executing queries\n",
    "#### Query (a): Simple aggregation\n",
    "Eqivalent to \n",
    "``` sql\n",
    "SELECT department, AVG(salary)\n",
    "FROM people_big\n",
    "GROUP BY department;\n",
    "```\n",
    "\n",
    "Parallel aggregation executed across worker threads\n",
    "\n",
    "#### Query (b): Nested aggregation\n",
    "This already uses an SQL-style query. I demonstrates multi-stage aggregation with intermediate result sets and Spark's ability to pipeline more complex queries.\n",
    "\n",
    "\n",
    "#### Query (c): Sorting + Top-N\n",
    "This is a global sort which is a shuffle-heavy operation. Spark can still parallelise it\n",
    "\n",
    "\n",
    "#### Query (d): Heavy self-join (COUNT only)\n",
    "This is the same quadratic explosion that was in the 2nd exercise. Spark can run this but it's still a better idea not to\n",
    "\n",
    "#### Query (d-safe): Join-equivalent rewrite\n",
    "Same result as previous but avoiding the join. Has a way lower complexity \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c97d5",
   "metadata": {},
   "source": [
    "## 2. Architectural contrasts with PostgreSQL\n",
    "|Feature|PostgreSQL|Spark|\n",
    "|----|----|----|\n",
    "|Architecture|Single-node (by default)|Distributed|\n",
    "|Scaling|Vertical|Horizontal|\n",
    "|Execution|Row-based|Partition-based|\n",
    "|Memory usage|Limited|In-memory first|\n",
    "|Best for|OLTP|OLAP / Analytics|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ad310",
   "metadata": {},
   "source": [
    "## 3. Advantages and limitations\n",
    "### Advantages\n",
    "- Scalability (Adding more nodes - more compute, can handle billions of rows)\n",
    "- Performance (In-memory computation, parallel aggregations, efficient shuffles)\n",
    "\n",
    "### Limitations\n",
    "- Overhead (Startup time, not ideal for small datasets)\n",
    "- Not OLTP(No transactions, indexes and constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2112cb5",
   "metadata": {},
   "source": [
    "## 4. Relation to Exercise 2\n",
    "### Exercise 2 showed that -\n",
    "- Naive analytical queries do not work well with OLTP databases at all\n",
    "- Usual solutions like views or idexes can helpa bit but are not enough\n",
    "\n",
    "### Exercise 3 shows that -\n",
    "- Distributed engines like spark can handle it better but still are not ideal\n",
    "- The best way to counter bad query desin is to rewrite it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c478c",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78cb3f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading orders from PostgreSQL ===\n",
      "Rows loaded: 1000000\n",
      "Load time: 3.14 seconds\n",
      "\n",
      "=== Query A ===\n",
      "+----------------+--------------+\n",
      "|product_category|price_per_unit|\n",
      "+----------------+--------------+\n",
      "|Automotive      |2000.0        |\n",
      "+----------------+--------------+\n",
      "\n",
      "Query A time: 8.09 seconds\n",
      "\n",
      "=== Query B ===\n",
      "+----------------+-------------------+\n",
      "|product_category|total_quantity_sold|\n",
      "+----------------+-------------------+\n",
      "|Health & Beauty |300842             |\n",
      "|Electronics     |300804             |\n",
      "|Toys            |300598             |\n",
      "+----------------+-------------------+\n",
      "\n",
      "Query B time: 5.17 seconds\n",
      "\n",
      "=== Query C ===\n",
      "+----------------+--------------------+\n",
      "|product_category|total_revenue       |\n",
      "+----------------+--------------------+\n",
      "|Automotive      |3.065897988599943E8 |\n",
      "|Electronics     |2.4152500945000267E8|\n",
      "|Home & Garden   |7.80237800900001E7  |\n",
      "|Sports          |6.1848990830000326E7|\n",
      "|Health & Beauty |4.65998178900003E7  |\n",
      "|Office Supplies |3.8276061640000574E7|\n",
      "|Fashion         |3.1566368219999947E7|\n",
      "|Toys            |2.3271039019999716E7|\n",
      "|Grocery         |1.5268355660000028E7|\n",
      "|Books           |1.273197603999989E7 |\n",
      "+----------------+--------------------+\n",
      "\n",
      "Query C time: 8.19 seconds\n",
      "\n",
      "=== Query D ===\n",
      "+--------------+-----------------+\n",
      "|customer_name |total_spent      |\n",
      "+--------------+-----------------+\n",
      "|Carol Taylor  |991179.1800000003|\n",
      "|Nina Lopez    |975444.9499999998|\n",
      "|Daniel Jackson|959344.4800000001|\n",
      "|Carol Lewis   |947708.5700000002|\n",
      "|Daniel Young  |946030.1400000004|\n",
      "|Alice Martinez|935100.0199999999|\n",
      "|Ethan Perez   |934841.2399999991|\n",
      "|Leo Lee       |934796.4799999993|\n",
      "|Eve Young     |933176.8599999989|\n",
      "|Ivy Rodriguez |925742.6400000005|\n",
      "+--------------+-----------------+\n",
      "\n",
      "Query D time: 8.31 seconds\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PostgresVsSparkBenchmark\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.2\")\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\n",
    "    .config(\"spark.history.fs.logDirectory\", \"/tmp/spark-events\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .config(\"spark.default.parallelism\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/postgres\"\n",
    "jdbc_props = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "print(\"\\n=== Loading orders from PostgreSQL ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_big = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"orders\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "\n",
    "# Force materialization\n",
    "row_count = df_big.count()\n",
    "\n",
    "print(f\"Rows loaded: {row_count}\")\n",
    "print(\"Load time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# Register temp view\n",
    "df_big.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# ============================================\n",
    "# 1. Query A\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query A ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_b = spark.sql(\"\"\"\n",
    "SELECT product_category, price_per_unit\n",
    "FROM orders\n",
    "ORDER BY price_per_unit DESC\n",
    "LIMIT 1;\n",
    "\"\"\")\n",
    "\n",
    "q_b.collect()\n",
    "q_b.show(truncate=False)\n",
    "print(\"Query A time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 2. Query B\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query B ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_b = spark.sql(\"\"\"\n",
    "SELECT product_category, SUM(quantity) AS total_quantity_sold\n",
    "FROM orders\n",
    "GROUP BY product_category\n",
    "ORDER BY total_quantity_sold DESC\n",
    "LIMIT 3;\n",
    "\"\"\")\n",
    "\n",
    "q_b.collect()\n",
    "q_b.show(truncate=False)\n",
    "print(\"Query B time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 3. Query C\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query C ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_b = spark.sql(\"\"\"\n",
    "SELECT product_category, SUM(price_per_unit * quantity) AS total_revenue\n",
    "FROM orders\n",
    "GROUP BY product_category\n",
    "ORDER BY total_revenue DESC;\n",
    "\"\"\")\n",
    "\n",
    "q_b.collect()\n",
    "q_b.show(truncate=False)\n",
    "print(\"Query C time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 4. Query D\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query D ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_b = spark.sql(\"\"\"\n",
    "SELECT customer_name, SUM(price_per_unit * quantity) AS total_spent\n",
    "FROM orders\n",
    "GROUP BY customer_name\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 10;\n",
    "\"\"\")\n",
    "\n",
    "q_b.collect()\n",
    "q_b.show(truncate=False)\n",
    "print(\"Query D time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
