services:
  # -------------------------
  # Kafka (KRaft mode)
  # -------------------------
  kafka:
    image: docker.io/bitnamilegacy/kafka:latest
    container_name: kafka
    restart: always
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      
      # 1. Define 3 listener names + Controller
      - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL_LOCAL://:9094,EXTERNAL_DOCKER://:9095,CONTROLLER://:9093
      
      # 2. Map each listener to the specific host/IP it should return to the client
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL_LOCAL://localhost:9094,EXTERNAL_DOCKER://host.docker.internal:9095
      
      # 3. All are using PLAINTEXT for this dev setup
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL_LOCAL:PLAINTEXT,EXTERNAL_DOCKER:PLAINTEXT,CONTROLLER:PLAINTEXT
      
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_KRAFT_CLUSTER_ID=kraft-cluster-1

    ports:
      - "9092:9092" # For Spark-client (INTERNAL)
      - "9094:9094" # For your Host machine (via localhost)
      - "9095:9095" # For Host machine (via host.docker.internal)
    networks:
      - streaming-net

  # -------------------------
  # Spark Master
  # -------------------------
  spark-master:
    image: bitnamilegacy/spark:4.0.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8080:8080"
    networks:
      - streaming-net

  # -------------------------
  # Spark Worker
  # -------------------------
  spark-worker:
    image: bitnamilegacy/spark:4.0.0
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
    networks:
      - streaming-net
    deploy:
      replicas: 1
      resources:
        limits:
          memory: 1024M  
          cpus: '1'

  # -------------------------
  # Spark Client
  # -------------------------
  spark-client:
    image: bitnamilegacy/spark:4.0.0
    container_name: spark-client
    user: root
    environment:
      - HADOOP_USER_NAME=spark
      - SPARK_SUBMIT_OPTS=-Dspark.jars.ivy=/tmp/.ivy2
    volumes:
      - ./logs-processing:/opt/spark-apps
    entrypoint: [ "sleep", "infinity" ]
    networks:
      - streaming-net
    ports:
      - "4040:4040" 


networks:
  streaming-net:
    name: streaming-net  # This makes the name predictable
    driver: bridge
